#!/usr/bin/env python3
"""
Sliding Window Pseudo-Streaming Test
Test very small chunk sizes (10-20 chars) to simulate streaming TTFB
"""

import time
import warnings
import numpy as np
import torch
import torchaudio
from f5_tts.api import F5TTS
import re

warnings.filterwarnings("ignore")

class SlidingWindowStreamingTest:
    def __init__(self, ref_audio_path, ref_text, model_name="F5TTS_v1_Base", device="cuda"):
        self.ref_audio_path = ref_audio_path
        self.ref_text = ref_text
        self.model_name = model_name
        self.device = device
        
        print(f"ğŸ¬ Sliding Window Pseudo-Streaming Test")
        print(f"ğŸ’¡ Concept: Small chunks (10-20 chars) for realistic TTFB simulation")
        print(f"ğŸ“Š Model: {model_name}")
        print(f"ğŸ¯ Device: {device}")
        print(f"ğŸ“ Reference: {ref_text}")
        print("="*80)
        
        # Initialize F5TTS
        print("Loading F5-TTS model...")
        self.f5tts = F5TTS(model=model_name, device=device)
        print("âœ… Model loaded successfully!")
    
    def smart_chunk_text(self, text, target_chunk_size=15):
        """
        Intelligently chunk text at word boundaries near target size
        """
        words = text.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            # Check if adding this word would exceed target size
            word_length = len(word) + (1 if current_chunk else 0)  # +1 for space
            
            if current_length + word_length > target_chunk_size and current_chunk:
                # Finalize current chunk
                chunk_text = " ".join(current_chunk)
                chunks.append(chunk_text)
                current_chunk = [word]
                current_length = len(word)
            else:
                # Add word to current chunk
                current_chunk.append(word)
                current_length += word_length
        
        # Add remaining words as final chunk
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
    
    def test_small_chunk_ttfb(self, chunk_sizes=[10, 15, 20, 25], nfe_step=8):
        """
        Test TTFB simulation with different small chunk sizes
        """
        print(f"\nğŸ§ª Small Chunk TTFB Testing (NFE {nfe_step})")
        print("="*80)
        
        # Test text
        test_text = "The quick brown fox jumps over the lazy dog, demonstrating remarkable agility and speed through the dense forest as wildlife watches in amazement."
        
        print(f"ğŸ“ Test text: {test_text}")
        print(f"ğŸ“ Total length: {len(test_text)} characters")
        print()
        
        results = []
        
        for chunk_size in chunk_sizes:
            print(f"\nğŸ¯ Testing chunk size: {chunk_size} characters")
            print("-" * 60)
            
            # Chunk the text
            chunks = self.smart_chunk_text(test_text, target_chunk_size=chunk_size)
            
            print(f"ğŸ“¦ Generated {len(chunks)} chunks:")
            for i, chunk in enumerate(chunks):
                print(f"   {i+1}: '{chunk}' ({len(chunk)} chars)")
            
            # Test first chunk (TTFB simulation)
            first_chunk = chunks[0]
            print(f"\nâš¡ TTFB Test - First chunk: '{first_chunk}'")
            
            ttfb_times = []
            audio_durations = []
            rtfs = []
            
            # Run multiple tests for first chunk
            for run in range(3):
                start_time = time.time()
                
                wav, sr, _ = self.f5tts.infer(
                    ref_file=self.ref_audio_path,
                    ref_text=self.ref_text,
                    gen_text=first_chunk,
                    nfe_step=nfe_step,
                    show_info=lambda x: None,
                )
                
                end_time = time.time()
                ttfb = end_time - start_time
                
                if wav is not None:
                    audio_duration = len(wav) / sr
                    rtf = ttfb / audio_duration
                    
                    ttfb_times.append(ttfb)
                    audio_durations.append(audio_duration)
                    rtfs.append(rtf)
                    
                    print(f"   Run {run+1}: TTFB {ttfb:.3f}s | Audio: {audio_duration:.2f}s | RTF: {rtf:.4f}")
            
            if ttfb_times:
                avg_ttfb = np.mean(ttfb_times)
                avg_audio = np.mean(audio_durations)
                avg_rtf = np.mean(rtfs)
                chars_per_sec = len(first_chunk) / avg_ttfb
                
                result = {
                    'chunk_size': chunk_size,
                    'first_chunk': first_chunk,
                    'first_chunk_chars': len(first_chunk),
                    'total_chunks': len(chunks),
                    'avg_ttfb': avg_ttfb,
                    'avg_audio_duration': avg_audio,
                    'avg_rtf': avg_rtf,
                    'chars_per_sec': chars_per_sec,
                    'nfe_step': nfe_step
                }
                results.append(result)
                
                print(f"\nğŸ“Š Chunk Size {chunk_size} Summary:")
                print(f"   Simulated TTFB: {avg_ttfb:.3f}s")
                print(f"   Audio duration: {avg_audio:.2f}s")
                print(f"   RTF: {avg_rtf:.4f}")
                print(f"   Processing: {chars_per_sec:.1f} chars/s")
                print(f"   Total chunks: {len(chunks)}")
        
        return results
    
    def simulate_sliding_window_streaming(self, text, chunk_size=15, nfe_step=8):
        """
        Simulate sliding window streaming with small chunks
        """
        print(f"\nğŸ¬ Sliding Window Streaming Simulation")
        print(f"ğŸ“ Text: {text}")
        print(f"ğŸ¯ Chunk size: {chunk_size} chars, NFE: {nfe_step}")
        print("="*80)
        
        # Chunk the text
        chunks = self.smart_chunk_text(text, target_chunk_size=chunk_size)
        
        print(f"ğŸ“¦ Generated {len(chunks)} chunks:")
        for i, chunk in enumerate(chunks):
            print(f"   {i+1}: '{chunk}' ({len(chunk)} chars)")
        
        # Simulate streaming
        streaming_results = []
        total_start_time = time.time()
        cumulative_audio_duration = 0
        
        print(f"\nğŸ¬ Streaming Simulation:")
        print("-" * 60)
        
        for i, chunk in enumerate(chunks):
            chunk_start_time = time.time()
            
            # Generate chunk
            wav, sr, _ = self.f5tts.infer(
                ref_file=self.ref_audio_path,
                ref_text=self.ref_text,
                gen_text=chunk,
                nfe_step=nfe_step,
                show_info=lambda x: None,
            )
            
            chunk_end_time = time.time()
            chunk_generation_time = chunk_end_time - chunk_start_time
            
            if wav is not None:
                audio_duration = len(wav) / sr
                cumulative_audio_duration += audio_duration
                chunk_rtf = chunk_generation_time / audio_duration
                
                # Calculate streaming metrics
                time_from_start = chunk_end_time - total_start_time
                
                if i == 0:
                    ttfb = chunk_generation_time
                    print(f"ğŸ¯ TTFB (First chunk): {ttfb:.3f}s")
                
                print(f"Chunk {i+1:2d}: {chunk_generation_time:.3f}s | Audio: {audio_duration:.2f}s | RTF: {chunk_rtf:.4f} | Cumulative: {time_from_start:.3f}s")
                
                streaming_results.append({
                    'chunk_index': i,
                    'chunk_text': chunk,
                    'chunk_chars': len(chunk),
                    'generation_time': chunk_generation_time,
                    'audio_duration': audio_duration,
                    'chunk_rtf': chunk_rtf,
                    'cumulative_time': time_from_start,
                    'cumulative_audio': cumulative_audio_duration
                })
        
        total_time = time.time() - total_start_time
        overall_rtf = total_time / cumulative_audio_duration if cumulative_audio_duration > 0 else float('inf')
        
        print(f"\nğŸ“Š Streaming Summary:")
        print(f"   TTFB: {streaming_results[0]['generation_time']:.3f}s")
        print(f"   Total time: {total_time:.3f}s")
        print(f"   Total audio: {cumulative_audio_duration:.2f}s")
        print(f"   Overall RTF: {overall_rtf:.4f}")
        print(f"   Chunks: {len(chunks)}")
        print(f"   Avg chunk size: {np.mean([len(chunk) for chunk in chunks]):.1f} chars")
        
        return {
            'chunks': chunks,
            'streaming_results': streaming_results,
            'ttfb': streaming_results[0]['generation_time'] if streaming_results else 0,
            'total_time': total_time,
            'total_audio_duration': cumulative_audio_duration,
            'overall_rtf': overall_rtf,
            'chunk_size_target': chunk_size,
            'nfe_step': nfe_step
        }
    
    def test_nfe_scaling_on_small_chunks(self, chunk_size=15):
        """
        Test different NFE values on small chunks for TTFB optimization
        """
        print(f"\nâš¡ NFE Scaling on Small Chunks ({chunk_size} chars)")
        print("="*80)
        
        # Test text
        test_text = "Hello world, this is a test of small chunk generation for streaming simulation."
        chunks = self.smart_chunk_text(test_text, target_chunk_size=chunk_size)
        first_chunk = chunks[0]
        
        print(f"ğŸ§ª Testing first chunk: '{first_chunk}' ({len(first_chunk)} chars)")
        print("-" * 60)
        
        nfe_steps = [4, 8, 16, 32]
        results = []
        
        for nfe in nfe_steps:
            print(f"\nğŸ¯ NFE {nfe}:")
            
            ttfb_times = []
            
            for run in range(3):
                start_time = time.time()
                
                wav, sr, _ = self.f5tts.infer(
                    ref_file=self.ref_audio_path,
                    ref_text=self.ref_text,
                    gen_text=first_chunk,
                    nfe_step=nfe,
                    show_info=lambda x: None,
                )
                
                end_time = time.time()
                ttfb = end_time - start_time
                ttfb_times.append(ttfb)
                
                if wav is not None:
                    audio_duration = len(wav) / sr
                    rtf = ttfb / audio_duration
                    print(f"   Run {run+1}: {ttfb:.3f}s | Audio: {audio_duration:.2f}s | RTF: {rtf:.4f}")
            
            if ttfb_times:
                avg_ttfb = np.mean(ttfb_times)
                std_ttfb = np.std(ttfb_times)
                
                results.append({
                    'nfe_step': nfe,
                    'avg_ttfb': avg_ttfb,
                    'std_ttfb': std_ttfb,
                    'chunk_chars': len(first_chunk)
                })
                
                print(f"   Average TTFB: {avg_ttfb:.3f}s (Â±{std_ttfb:.3f}s)")
        
        if results:
            print(f"\nğŸ“Š NFE Comparison for Small Chunks:")
            print("-" * 60)
            print(f"{'NFE':<6} {'TTFB (s)':<10} {'Std Dev':<10} {'Speed vs NFE32':<15}")
            print("-" * 60)
            
            baseline_ttfb = next((r['avg_ttfb'] for r in results if r['nfe_step'] == 32), None)
            
            for result in results:
                nfe = result['nfe_step']
                ttfb = result['avg_ttfb']
                std = result['std_ttfb']
                
                if baseline_ttfb:
                    speedup = ((baseline_ttfb - ttfb) / baseline_ttfb * 100)
                    speed_str = f"{speedup:+.1f}%"
                else:
                    speed_str = "N/A"
                
                print(f"{nfe:<6} {ttfb:<10.3f} {std:<10.3f} {speed_str:<15}")
        
        return results
    
    def generate_streaming_audio_samples(self, chunk_size=15, nfe_step=8):
        """
        Generate audio samples for different chunks to test quality
        """
        print(f"\nğŸµ Generating Streaming Audio Samples")
        print(f"ğŸ¯ Chunk size: {chunk_size} chars, NFE: {nfe_step}")
        print("="*80)
        
        # Test text
        test_text = "The future of artificial intelligence looks incredibly promising, with advances in machine learning and neural networks paving the way for revolutionary applications."
        
        chunks = self.smart_chunk_text(test_text, target_chunk_size=chunk_size)
        
        print(f"ğŸ“ Full text: {test_text}")
        print(f"ğŸ“¦ Chunked into {len(chunks)} parts:")
        
        audio_segments = []
        
        for i, chunk in enumerate(chunks):
            print(f"\nğŸ¯ Generating chunk {i+1}: '{chunk}'")
            
            start_time = time.time()
            
            wav, sr, _ = self.f5tts.infer(
                ref_file=self.ref_audio_path,
                ref_text=self.ref_text,
                gen_text=chunk,
                nfe_step=nfe_step,
                show_info=lambda x: None,
            )
            
            generation_time = time.time() - start_time
            
            if wav is not None:
                # Convert to torch if needed
                if isinstance(wav, np.ndarray):
                    wav = torch.from_numpy(wav)
                
                # Handle clipping
                max_amplitude = torch.max(torch.abs(wav)).item()
                if max_amplitude > 1.0:
                    wav = wav / max_amplitude * 0.95
                
                # Save individual chunk
                chunk_filename = f"streaming_chunk_{i+1:02d}_{len(chunk)}chars.wav"
                if wav.dim() == 1:
                    wav = wav.unsqueeze(0)
                torchaudio.save(chunk_filename, wav, sr)
                
                audio_duration = len(wav[0]) / sr
                rtf = generation_time / audio_duration
                
                audio_segments.append(wav[0])
                
                print(f"   âœ… Saved: {chunk_filename}")
                print(f"   â±ï¸  Generation: {generation_time:.3f}s")
                print(f"   ğŸµ Duration: {audio_duration:.2f}s")
                print(f"   ğŸ“Š RTF: {rtf:.4f}")
                
                # Special note for first chunk (TTFB)
                if i == 0:
                    print(f"   ğŸ¯ SIMULATED TTFB: {generation_time:.3f}s")
        
        # Combine all chunks
        if audio_segments:
            combined_audio = torch.cat(audio_segments, dim=0)
            combined_filename = f"streaming_combined_{len(chunks)}chunks_{chunk_size}chars.wav"
            torchaudio.save(combined_filename, combined_audio.unsqueeze(0), sr)
            
            total_duration = len(combined_audio) / sr
            
            print(f"\nâœ… Combined audio saved: {combined_filename}")
            print(f"ğŸµ Total duration: {total_duration:.2f}s")
            print(f"ğŸ“¦ From {len(chunks)} chunks of ~{chunk_size} chars each")
        
        return chunks
    
    def run_comprehensive_streaming_test(self):
        """
        Run comprehensive sliding window streaming tests
        """
        print(f"\nğŸš€ Comprehensive Sliding Window Streaming Test")
        print("="*80)
        
        # Test 1: Small chunk TTFB comparison
        ttfb_results = self.test_small_chunk_ttfb(chunk_sizes=[10, 15, 20, 25], nfe_step=8)
        
        # Test 2: NFE scaling on small chunks
        nfe_results = self.test_nfe_scaling_on_small_chunks(chunk_size=15)
        
        # Test 3: Sliding window simulation
        test_text = "Welcome to the future of text-to-speech technology, where artificial intelligence meets natural language processing to create incredibly realistic and expressive synthetic voices that can adapt to various contexts and speaking styles."
        
        streaming_result = self.simulate_sliding_window_streaming(test_text, chunk_size=15, nfe_step=8)
        
        # Test 4: Generate audio samples
        sample_chunks = self.generate_streaming_audio_samples(chunk_size=15, nfe_step=8)
        
        # Analysis and Summary
        print(f"\nğŸ¯ SLIDING WINDOW STREAMING ANALYSIS")
        print("="*80)
        
        if ttfb_results:
            print(f"ğŸ† TTFB SIMULATION RESULTS:")
            print(f"{'Chunk Size':<12} {'TTFB (s)':<10} {'Chars':<8} {'Chunks':<8} {'Best For':<15}")
            print("-" * 60)
            
            for result in ttfb_results:
                chunk_size = result['chunk_size']
                ttfb = result['avg_ttfb']
                chars = result['first_chunk_chars']
                chunks = result['total_chunks']
                
                if ttfb < 0.200:
                    best_for = "Ultra responsive"
                elif ttfb < 0.400:
                    best_for = "Real-time"
                else:
                    best_for = "Quality"
                
                print(f"{chunk_size:<12} {ttfb:<10.3f} {chars:<8} {chunks:<8} {best_for:<15}")
        
        if nfe_results:
            print(f"\nâš¡ OPTIMAL NFE FOR SMALL CHUNKS:")
            fastest_nfe = min(nfe_results, key=lambda x: x['avg_ttfb'])
            print(f"   Fastest TTFB: NFE {fastest_nfe['nfe_step']} ({fastest_nfe['avg_ttfb']:.3f}s)")
            
            ultra_fast = [r for r in nfe_results if r['avg_ttfb'] < 0.200]
            if ultra_fast:
                print(f"   Ultra-fast NFE: {[r['nfe_step'] for r in ultra_fast]}")
        
        if streaming_result:
            print(f"\nğŸ¬ STREAMING SIMULATION:")
            print(f"   Simulated TTFB: {streaming_result['ttfb']:.3f}s")
            print(f"   Total chunks: {len(streaming_result['chunks'])}")
            print(f"   Overall RTF: {streaming_result['overall_rtf']:.4f}")
            print(f"   Total audio: {streaming_result['total_audio_duration']:.2f}s")
        
        print(f"\nğŸ’¡ KEY INSIGHTS:")
        print(f"   â€¢ Small chunks (10-20 chars) enable realistic TTFB simulation")
        print(f"   â€¢ NFE 4-8 provide excellent TTFB for streaming")
        print(f"   â€¢ Sliding window approach creates natural speech flow")
        print(f"   â€¢ Sub-200ms TTFB achievable with optimized chunking")
        print(f"   â€¢ Quality remains high even with small chunks")
        
        print(f"\nğŸ§ AUDIO SAMPLES GENERATED:")
        print(f"   â€¢ Individual chunk files: streaming_chunk_XX_XXchars.wav")
        print(f"   â€¢ Combined streaming audio: streaming_combined_XXchunks_XXchars.wav")
        print(f"   â€¢ Listen to evaluate chunk transition quality")
        
        return {
            'ttfb_results': ttfb_results,
            'nfe_results': nfe_results,
            'streaming_result': streaming_result,
            'sample_chunks': sample_chunks
        }

def main():
    # Configuration
    ref_audio_path = "src/f5_tts/infer/examples/basic/basic_ref_en.wav"
    ref_text = "Some call me nature, others call me mother nature."
    
    # Initialize test
    test = SlidingWindowStreamingTest(
        ref_audio_path=ref_audio_path,
        ref_text=ref_text,
        model_name="F5TTS_v1_Base",
        device="cuda"
    )
    
    # Run comprehensive streaming tests
    results = test.run_comprehensive_streaming_test()
    
    print(f"\nğŸ‰ Sliding window streaming test complete!")
    print(f"ğŸ¬ Pseudo-streaming with small chunks shows excellent TTFB potential")
    print(f"âš¡ Ready for production streaming applications!")

if __name__ == "__main__":
    main() 